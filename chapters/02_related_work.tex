% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Related work}\label{chapter:relatedwork}

\section{Background}
In the pipeline of query execution, cardinality estimation serves 
as a cornerstone for the optimization process. 
Cardinality, defined as the number of tuples in the output, 
plays a pivotal role in the selection of an optimal query plan. 
Modern Database Management Systems (DBMSs) often rely on 
cost-based Query Optimizers to make this selection. 
For example, the SQL Server Query Optimizer
 \parencite{microsoft2023cardinality} employs a 
 cost-based approach, aiming to minimize the estimated 
 processing cost of executing a query.

The cost estimation is influenced by two primary factors: 
the cardinality of the query plan and the algorithmic cost 
model dictated by the operators used in the query. The 
former serves as an input parameter for the latter, creating 
a dependency between the two. Enhanced cardinality estimation 
can lead to more accurate cost models, which in turn results 
in more efficient query execution plans.

The quality of the cardinality estimate is contingent 
upon its precision. Ideally, one seeks a tight upper bound estimated
 that is subject to minimal constraints and assumptions.
  This ensures that the Query Optimizer can make more accurate 
  and effective decisions, thereby improving the overall 
  performance of the DBMS.

\subsection{Cardinality Estimation}
As previously discussed, accurate and 
reliable cardinality estimates are crucial 
in achieving faster query execution times. 
The objective is to develop a Linear Programming (LP) 
solver designed specifically for cardinality estimation. 
This solver aims to maximize a cost function that represents 
the upper bound of the output size, optimizing for both 
time and memory complexity. The performance of this solver 
will be evaluated using the Query-per-Hour Performance Metric 
(QphH@Size).

To set the stage for our implementation, 
we focus on the problem of upper-bounding the 
cardinality of a join query $Q$. 
We begin by considering the worst-case join size and then proceed
to add some constraints that keep some of our main variables in check.
 The motivation behind this work is to frame the problem of 
 estimating the size of a multi-join query as a packing 
 linear programming problem.

To elucidate the core concepts, 
let's consider a simple example 
where the query is a join operation between two relations.
\[
Q(a, b, c) = R(a, b) \land S(b, c)
\]
where we denote the sizes of the relations as
$|R|$ and $|S|$ respectively.
It is easy to see that the largest possible output is $N1 \cdot N2$, which occurs when the join
behaves like a cartesian product. So, this is the worst-case
upper bound.
Now the maximum sizes of these relations (i.e. the number of tuples, in our case pairs) depend on the
variables $a$, $b$ and $c$, their respective types and domains. It can also
be affected by te nature of the data or business rules.
We start with the inequality \ref{eq:initial_inequality}. Applying the natural logarithm to both sides yields \ref{eq:log_inequality}. We then rename the variables, simplifying the inequality to \ref{eq:renamed_inequality}. 
Normalizing by dividing both sides by \(r'\), we obtain \ref{eq:normalized_inequality}. This leads us to the objective function for our packing LP problem.
\begin{align}
    |a| \cdot |b| &\leq |R| \label{eq:initial_inequality} \\
    \ln|a| + \ln|b| &\leq \ln|R| \label{eq:log_inequality} \\
    a' + b' &\leq r' \label{eq:renamed_inequality} \\
    \frac{1}{r'} a' + \frac{1}{r'} b' &\leq 1 \label{eq:normalized_inequality} \\
    \text{maximize } a' + b' + c' + d' \quad &\text{s.t.} \quad \frac{1}{r'} a' + \frac{1}{r'} b' \leq 1 \label{eq:objective_function}
    \end{align}

And in this simple abstracted way we get a sample packing LP from our dataset.
\subsection{Linear Programming}
The LP problem class that we are dealing with is called the packing LP problem. Additionally
it is a special instance where:
\begin{itemize}
    \item \( c \), the vector of the variable coefficients in the objective function,
    is a vector of all ones
    \item \( b \) , or the right hand side vector,  is a vector of all ones
\end{itemize}
Our specific problem is then expressed as follows:
\begin{align}
    \text{Maximize} \quad & \sum_{j=1}^{n} x_j \notag \\
    \text{subject to} \quad & \notag \\
    & \sum_{j=1}^{n} a_{ij} x_j \leq 1, \quad & i = 1, \ldots, m \notag \\
    & x_j \geq 0, \quad & j = 1, \ldots, n \label{LP_Problem}
\end{align}
\begin{itemize}
    \item \( x_j \) is the \( j^{th} \) decision variable.
    \item \( m \) is the number of constraints.
    \item \( n \) is the number of variables.
\end{itemize}
This specific class of LPs has a simple structure that we can exploit
to further optimize our implementation.


\subsection{The Standard Simplex Algorithm}
\subsubsection{The algorithm}
In this subsection we will present the most widely used algorithm for solving
LP problems. We have implemented our version  of this algorithm in the C++ language 
and we use it, among others, to solve our dataset.
To be approachable by the simplex algorithm, the LP \ref*{LP_Problem} needs to be cast in a 
computational, or tabular form, that fulfills the requirement of the constraint matrix having to have
full row rank and only equality  constraints are allowed. This is done by introducing slack
variables. 
A simple packing LP in tabular form would look like this:
\begin{equation*}
    \begin{array}{ccccccc|c}
      & a & b & c & d & s_1 & s_2 & \text{RHS} \\
      \hline
      z & -1 & -1 & -1 & -1 & 0 & 0 & 0 \\
      \hline
      s_1 & a_{11} & a_{12} & a_{13} & a_{14} & 1 & 0 & 1 \\
      s_2 & a_{21} & a_{22}& a_{23} & a_{24}& 0 & 1 & 1\\
    \end{array}
\end{equation*}
We call this a feasible dictionary\parencite{chvatal1983linear}, or a feasible tableau. This is apparent, since all
values in the RHS are positive, and the constraint matrix $A$ also has positive coefficients.
\begin{itemize}
    \item feasible dictionaries? 
    \item just write the algorithm in abstarct way, in the approach chapter write it in the 
    specific way I implemented it (Bland's rule, zero tolerance, ...)
    \item the grand strategy of the simplex method is that of successive improvements
    \item decision variables vs. slack variables
    \item A maximization problem is optimized when the slack variables are “squeezed out,” maximizing the true variables’ effect on the objective function. Conversely, a minimization problem is optimized when the slack variables are “stuffed,” 
    minimizing the true variables’ effect on the objective function.
    \item feasibility, boundedness, 
    \item largest coefficient rule vs. largest increase rule.
    \item the problem of stalling, degeneracy
    \item Bland's rule guarantees termination.
    
\end{itemize}

\subsubsection{The complexity}
The Simplex algorithm for linear programming has an 
exponential worst-case time complexity, which we denote by \( O(2^n) \).
For packing linear programs, the worst-case time complexity of the Simplex algorithm 
remains exponential, even though there exists polynomial time implementations for it.
\parencite{stille2010solution}.


\subsection{The Revised Simplex Algorithm}
As explained in the book \parencite{chvatal1983linear}. 
while Standard Simplex algorithm maintains and updates the entire tableau in its dense form,
 which includes both basic and non-basic variables, at each iteration. 
 In contrast, the Revised Simplex algorithm focuses on tracking only 
 the basic variables and their corresponding basis matrix $B$, 
 thereby reducing memory requirements. 
 The Revised Simplex algorithm often employs 
 the product form update method to efficiently 
 update the inverse of the basis matrix $B^{-1}$ 
 without having to recompute it from scratch, 
 making it computationally more efficient for large-scale problems. 
 Finally, we primarily use the Compressed Column Representation (CCR) of the, 
 which greatly benefits the performance of the implementation.
\begin{algorithm}
    \caption{Revised Simplex Algorithm}
    \begin{enumerate}
        \item \textbf{Input:} A feasible basic solution, \( B \), \( c \), \( A \), and \( b \)
        \item \textbf{Output:} Optimal solution or a certificate of unboundedness
        \item Initialize \( B^{-1} \), the inverse of the basis matrix \( B \)
        \item \textbf{While True:}
        \begin{itemize}
            \setlength{\itemindent}{3em}
            \item[\textit{Step 1:}] Solve the system \( yB = c_B \) (BTRAN)
            \item[\textit{Step 2:}] Choose an entering column. This may be any column a of
            $A_N$ such that $ya$ is less than the corresponding component
            of $c_N$. If there is no such column, then the current solution is optimal.
            In other words: Choose first \( j \) such that \( c_j -yA_j > 0 \) 
            then $a=A_j$ is the enterig column.
            \item[\textit{Step 3:}] Solve the system $Bd = a$ (FTRAN)
            \item[\textit{Step 4:}] Let $x_B^{\ast} = B^{-1}b$ the current basic variables' values.
            Find the largest $t$ such that \( x_B^{\ast} - td \geq 0\)
                if there is no such $t$, then the problem is unbounded; otherwise, at least 
                one component of  \( x_B^{\ast} - td \) equals zero and the corresponding variable is leaving the basis.
            \item[\textit{Step 5:}] Set the value of the entering variable at 
            $t$ and replace the values $x_B^{\ast}$ of the basic variables by \( x_B^{\ast} - td \).
            Replace the leaving column of B by the entering column, and in the basis heading,
            replace the leaving variable by the entering variable.
        \end{itemize}
        
        \item \textbf{Return} Optimal solution \( B^{-1}b \)
    \end{enumerate}
    \end{algorithm} 

\subsubsection*{The product form update method}
We will discuss the PFI, introduced by George Dantzig \parencite{dantzig1954product}.
\subsubsection*{Data structures}
Compressed Storage Formats:
We use the CSC format to store sparse matrices. These formats store the non-zero values, along with their corresponding row and column indices, 
in a compact way. This reduces memory usage and speeds up operations on sparse matrices

\begin{verbatim}
    struct CCRMatrix {
        float *values;  // Non-zero values in the matrix
        int *rowIdx;  // Row indices corresponding to the non-zero values
        int *colPtr;  // Points to the index in `values` where each column starts
    };
\end{verbatim}
\section{Previous Work}
Here we will discuss alternative approaches that are superseded by my work.
\subsection{Comparative studies of different update methods}
We will focus on one study \parencite{huangfu2015novel}.
\subsection{Other techniques}
The primal simplex method starts from a trial point that is primal feasible and iterates until dual feasibility.
The dual simplex method starts from a trial point that is dual feasible and iterates until primal feasibility.
ALGLIB implements a three-phase dual simplex method with additional degeneracy-breaking perturbation:
\begin{itemize}
    \item Forrest-Tomlin updates for faster LU refactorizations
    \item A bound flipping ratio test (also known as long dual step) for longer steps
    \item Dual steepest edge pricing for better selection of the leaving variable
    \item Shifting (dynamic perturbations applied to cost vector) for better stability
\end{itemize}
