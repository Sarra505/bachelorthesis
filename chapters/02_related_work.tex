% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Related work}\label{chapter:relatedwork}

\section{Background}
In this chapter we will talk about optimization, in particular the field
of linear programming. We will elaborate on
the most widely used algorithms and techniques to tackle this problem,
and we present some use cases and benchmarks.
A major use case of linear programming solvers is cardinality estimation, which
is a crucial step in the pipeline of query optimization. We will present
the background and related work needed to understand our contribution.

\subsection{Linear Programming}
Informally, \gls{lp} is a method to calculate the best possible outcome from a given set
of requirements. A concrete real-world application of such a method is
for instance aiming to maxmize profit in a business, given some constraints on your variables
like raw material availability, labor hours, etc.

Formally, \gls{lp} is a mathematical modeling technique in which
a linear function (called the objective function) \( z: \mathbb{R}^n \to \mathbb{R} \)
is maximized or minimized when subject to a set of linear constraints or inequalities.
A maximization \gls{lp} problem is then defined as:
\begin{align}
    \text{Maximize} \quad   & z = \mathbf{c}^T \mathbf{x} \notag            \\
    \text{subject to} \quad & \mathbf{A} \mathbf{x} \leq \mathbf{b} \notag  \\
                            & \mathbf{x} \geq \mathbf{0} \label{LP_Problem}
\end{align}
Where $n$ is the number of decision variables and $m$ is the number of constraints:
\(\mathbf{x} \in \mathbb{R}^n\) is the column vector of decision variables.
\(\mathbf{c} \in \mathbb{R}^n\) is the column vector of coefficients in the objective function.
\(\mathbf{A} \in \mathbb{R}^{m \times n}\) is the coefficient matrix in the constraints.
\(\mathbf{b} \in \mathbb{R}^m\) is the column vector of the right-hand sides of the constraints.
In the following sections, we focus on \gls{lp} problems that are maximization problems and we primarily
use the matrix representation of the problem.

To derive the setting for our contribution,
we also explore a special instance of \gls{lp} problems called packing \gls{lp}.
\subsubsection{Packing LP}
One LP problem class that we are dealing with is called the packing LP problem. It is a special instance where:
\( \mathbf{c}  = \mathbf{b} =  \begin{bmatrix}
    1 & 1 & \dots & 1
\end{bmatrix} \).
Our specific problem is then expressed as follows:
\begin{align}
    \text{Maximize} \quad   & \sum_{i=1}^{n} x_j \notag                                                             \\
    \text{subject to} \quad & \notag                                                                                \\
                            & \mathbf{A} \mathbf{x} \leq \mathbf{1}_m, \quad                                        \\
                            & x_i \geq 0, \quad                              & i = 1, \ldots, n \label{PLP_Problem}
\end{align}
Where $\mathbf{1}_m = \begin{bmatrix}
        1      \\
        1      \\
        \vdots \\
        1      \\
    \end{bmatrix}$


This specific class of LPs has a simple structure that we can exploit, see
Chapter \ref*{chapter:linearprogramming},
to further optimize our implementation.

\subsection{Duality}\label{duality}

The duality theorem is an interesting result in linear programming, that
states that very instance of maximization problem has a corresponding
minimization problem called its dual problem.
The two problems are linked in an interesting way:
if one problem has an optimal solution, then so does the other,
and their optimal solutions are equal.

For instance, consider the primal \gls{lp} and its dual problem on the right:
\[
    \begin{array}{c@{\quad}c@{\quad}c}
        \begin{aligned}
            \text{maximize} \quad   & \mathbf{c}^T \mathbf{x}               \\
            \text{subject to} \quad & \mathbf{A} \mathbf{x} \leq \mathbf{b} \\
                                    & \mathbf{x} \geq 0                     \\
        \end{aligned}
         &
        \begin{array}{c}
            \longrightarrow
        \end{array}
         &
        \begin{aligned}
            \text{minimize} \quad   & \mathbf{b}^T \mathbf{y}                 \\
            \text{subject to} \quad & \mathbf{A}^T \mathbf{y} \geq \mathbf{c} \\
                                    & \mathbf{y} \geq 0                       \\
        \end{aligned}
    \end{array}
\]

\subsubsection{Geometric Interpretation}
The linear programming problem can be understood geometrically as follows:
The linear constraints constitute the vertices (corners) of a polytope
defined by the feasible region the problem.
The simplex algorithm starts at an initial vertex and moves along
the edges of the polytope to vertices with better objective
values until the optimal solution is reached.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                xlabel=\(x_1\),
                ylabel=\(x_2\),
                xmin=0, xmax=10,
                ymin=0, ymax=10,
                grid=both,
                minor tick num=1,
                major grid style={gray!30},
                minor grid style={gray!10},
                axis lines=middle,
                enlargelimits={abs=0.5},
                legend pos=north east,
            ]

            % Constraints
            \addplot[blue, fill=blue!10, domain=0:8] {8 - 0.5*x} \closedcycle; % Example: x1 + 2x2 <= 8
            \addplot[red, fill=red!10, domain=0:6] {6 - x} \closedcycle; % Example: x1 + x2 <= 6

            % Objective function (for illustration, let's assume c1=2 and c2=3 and z=18)
            \addplot[green, domain=0:9, thick, dashed] {(18 - 2*x)/3}; % Example: 2x1 + 3x2 = 18

            % Arrow for objective function
            \draw[->, thick, green] (4,10/3) -- (5,6); % Arrow for the objective function

            \legend{
                \(a_{11}x_1 + a_{12}x_2 \leq b_1\),
                \(a_{21}x_1 + a_{22}x_2 \leq b_2\),
                Objective function
            }

        \end{axis}
    \end{tikzpicture}
    \caption{Graphical representation of the LP problem with directional arrows.}
    \label{fig:lp_problem}
\end{figure}


\subsection{Feasibility, unboundedness}\label{feasibility}

\subsection{The Standard Simplex Algorithm}
In this subsection we will present the most widely used algorithm for solving
LP problems, the simplex algorithm, as introduced by George Dantzig in 1947
\parencite{dantzig1990origins}. We have implemented our version of this algorithm
in the C++ language and we use it, among others, to solve our dataset.

\subsubsection{The algorithm}
To be approachable by the simplex algorithm, the \gls{lp} problem
\ref{LP_Problem} needs to be cast in a
computational or standard 7form \ref{LP_Problem_Comp}, that fulfills the requirement of the constraint matrix having to have
full row rank and only equality  constraints are allowed.
To convert the inequalities to equations, we introduce slack variables
\(s_1, s_2, \dots, s_m\). After introducing those variables
let's look at the problem \ref{LP_Problem}, where the constraints are now linear equalities:

\begin{align}
    \text{Maximize} \quad   & z = \mathbf{c}^T \mathbf{x} \notag                            \\
    \text{subject to} \quad & \sum_{j=1}^{n} a_{1j} x_j + s_1 = b_1 \notag                  \\
                            & \sum_{j=1}^{n} a_{2j} x_j + s_2 = b_2 \notag                  \\
                            & \vdots \notag                                                 \\
                            & \sum_{j=1}^{n} a_{mj} x_j + s_m = b_m \label{LP_Problem_Comp} \\
                            & x_1, x_2, \dots, x_n, s_1, s_2, \dots, s_m \geq 0 \notag
\end{align}

We then have a \gls{lp} problem in the appropriate form and can be used as input for
the simplex algorithm. To develop an intuition for how this algorithm works,
it is helpful to view the strategy of the simplex algorithm as that of
successive improvements until reaching an optimum. For instance, a maximization problem is optimized when the slack
variables are “squeezed out,” maximizing the true variables’ effect on the objective
function. Conversely, a minimization problem is optimized when the slack variables
are “stuffed,” minimizing the true variables’ effect on the objective function.

We already defined the concepts of feasibility and unboundedness.
We then will define what a basis is. A feasible basis is a set containing the basic 
variables and their values. Basic variables are set to 0 in the linear constraints.
For example an initial feasible basis for the problem, is the set of the decision variables.
Meaning we set all the decision variables in the linear constraints to zero and 
calculate the nonbasis variables' values 


The simplex method is first initialized by
an initial feasible solution $\mathbf{\bar{x}}$, which is a vector of nonnegative numbers.
This constitutes a feasible dictionary (or tableau), formally defined in \parencite{chvatal1983linear}.
The simplex method then contructs a sequence of feasible dictionaries until reaching an optimum.
This is how a simplex algorithm broadly looks like:

% basic and non basic variables !!!!! you have to explain what a basis is !!!!!!!!
\begin{algorithm}
    \caption{Simplex Algorithm}
    \begin{algorithmic}[1]
        \Procedure{Simplex}{$\mathbf{c}, \mathbf{A}, \mathbf{b}$}
        \State Initialize a feasible basic solution
        \If{no entering variable with positive reduced cost exists}
        \State \Return "Optimal solution found"
        \EndIf
        \If{no positive pivot element in the column}
        \State \Return "Unbounded"
        \EndIf
        \State Choose a leaving variable using the minimum ratio test
        \State Perform a pivot operation
        \State Update the basic and non-basic variables
        \State \Return current basic solution
        \EndProcedure
    \end{algorithmic}
\end{algorithm}


We call this a feasible dictionary\parencite{chvatal1983linear}, or a feasible tableau. This is apparent, since all
values in the RHS are positive, and the constraint matrix $A$ also has positive coefficients.

Let's present the methods used to perform the exchange in each step, i.e. the choice of the entering
variable and the choice of the leaving variable.

\begin{itemize}
    \item The choice of the entering variable: we choose a non-basic variable to enter the basis
     and thus become basic. This is called Pricing. The choice usually depends on metrics 
     like the largest increase in the objective function, or the largest coefficient. Bland'S
     rule has been proved to guarantee termination.

    \item The choice of the leaving variable: we choose a basic variable to leave the basis:
    we do this by performing a ratio test.
\end{itemize}
\subsubsection{Termination}

\subsubsection{The runtime complexity}
%Worst case vs. average case
With $m<50$ and $m+n<200$, where $m$ and $n$ are the number of constraints and variables in the \gls{lp}
problem respectively, Dantzig observed that the number of iterations are usually less than $3m/2$
and only rarely going to $3m$. However, there is no proof that for every
problem the simplex algorithm for
linear programming has a number of iterations or pivots that
is majorized by a polynomial function.
In fact, Klee and Minty (1972) \parencite{klee1972good} constructed a worst-case example where
$2^m -1$ iterations may be required, making the simplex'
worst-case time complexity exponential, which we denote by \( O(2^m) \).
It can be however argued that this is only one worst-case example. Indeed, the number
of iterations usually encountred in practice or even in formal experimental studies of is much lower.
For packing linear programs, the worst-case time complexity of the Simplex algorithm
remains exponential, even though there exists polynomial time implementations for it.
\parencite{stille2010solution}.

\subsection{Time complextity analysis of one iteration of the tableau simplex algorithm}
\subsection{The interior point method}

\subsection{Revised Simplex Algorithm}
While the standard or tableau simplex algorithm maintains and updates the entire tableau in its
dense form at each iteration, and the pivotting step of this algorithm is 
highly costlywe have to update the entire matrix
using row operations, the revised simplex
method transforms only the inverse of the basis matrix, $mathbb{B}^{-1}$, thus
reducing the amount of writing at each step and overall memory usage.
This is explained in the following mathematical proof.
\subsubsection*{The algorithm}
Let's derive the mathematical proof of this algorithm, and elucidate the
equivalency between Tableau and Revised simplex, and introduce the
speedup the latter brings.
Given a linear programming problem in standard form:
\begin{align*}
    \text{maximize} \quad   & \mathbb{c}^T \mathbb{x}    \\
    \text{subject to} \quad & \mathbb{A}\mathbb{x} = \mathbb{b}   \\
                            & \mathbb{x} \geq \mathbb{0}
\end{align*}
where \( A \) is an \( m \times n \) matrix,
\( b \) is an \( m \times 1 \) vector, and
\( c \) is an \( n \times 1 \) vector.


Partition \( x \) into basic (\( x_B \)) and non-basic (\( x_N \)) variables.
Similarly, partition \( A \) into \( B \) (columns corresponding to \( x_B \))
and \( N \) (columns corresponding to \( x_N \)).

The constraints can be written as:
\begin{align*}
    Bx_B + Nx_N & = b    \\
    x_B         & \geq 0 \\
    x_N         & \geq 0
\end{align*}

From \( Bx_B + Nx_N = b \), when \( x_N = 0 \):
\[ x_B = B^{-1}b \]
This is the basic feasible solution if all entries of \( x_B \) are non-negative.

Compute the reduced costs:
\[ \bar{c}_N^T = c_N^T - c_B^T B^{-1} N \]
If all entries of \( \bar{c}_N^T \) are non-negative,
then the current basic feasible solution is optimal.

If some entries of \( \bar{c}_N^T \) are negative,
choose \( j \) such that \( \bar{c}_j < 0 \). Compute:
\[ d = B^{-1} A_j \]
If all entries of \( d \) are non-positive, the problem is unbounded.

Otherwise, compute the step length:
\[ \theta = \min \left\{ \frac{x_B[i]}{d[i]} : d[i] > 0 \right\} \]
Update the solution:
\[ x_B = x_B - \theta d \]
\[ x_j = \theta \]
and adjust the sets of basic and non-basic variables.

Repeat the optimality test, and if necessary, the pivot operation,
until an optimal solution is found or the problem is determined to be unbounded.

This proof elucidates that at every stage of the simplex method, we only have to
track the following, and with this only we are able to "recreate" exactly the tableau at
each step, without performing the costly pivotting operation:
\begin{itemize}
    \item the indices of basic and non-basic variables
    \item $B^{-1}$ the inverse of the basis matrix. This is used to solve two types of linear equations
          during an iteration, see step 1 and 3 in \ref{algo:revised}.
    \item the current values of the basic variables, or the current basic feasible solution $x_B = B^{-1}b$
\end{itemize}
In practical terms, we only update the basis matrix every iteration, and we will be able to use it to
perform all the steps needed to update our problem data and go on to the next
iteration.
The algorithm is elaborated in \ref{algo:revised}. As we can see, step 1 and 3 represent the solving of
two types of systems , \gls{ftran} and \gls{btran}. This can be done using a multiplication with
the basis matrix inverse'.

However, having to recompute the inverse of a matrix is costly.

For a square matrix of size \(n \times n\), the time complexity of LU decomposition
\parencite{golub2013matrix},
which is one of the
most prominent methods to invert a matrix is $O(n^3) $.

This is why it is desirable to employ another tool to efficiently
update the inverse of the basis matrix $B^{-1}$
without having to recompute it from scratch,
making it computationally more efficient for large-scale problems. Such tools are called update methods,
and we will later describe the \gls{pfi}, the modified \gls{pfi} and Forrest-Tomlin update method.

Finally, in our implementation we also use the \gls{ccr} format to store matrices,
see \ref{subsubsection:sparse-matrix} of the,
which greatly benefits the performance of the implementation.

\begin{algorithm}
    \caption{Revised Simplex Algorithm}
    \begin{enumerate}
        \item \textbf{Input:} A feasible basic solution, \( B \), \( c \), \( A \), and \( b \)
        \item \textbf{Output:} Optimal solution or a certificate of unboundedness
        \item Initialize \( B^{-1} \), the inverse of the basis matrix \( B \)
        \item \textbf{While True:}
              \begin{itemize}
                  \setlength{\itemindent}{3em}
                  \item[\textit{Step 1:}] Solve the system \( yB = c_B \) (BTRAN)
                  \item[\textit{Step 2:}] Choose an entering column. This may be any column a of
                      $A_N$ such that $ya$ is less than the corresponding component
                      of $c_N$. If there is no such column, then the current solution is optimal.
                      In other words: Choose first \( j \) such that \( c_j -yA_j > 0 \)
                      then $a=A_j$ is the enterig column.
                  \item[\textit{Step 3:}] Solve the system $Bd = a$ (FTRAN)
                  \item[\textit{Step 4:}] Let $x_B^{\ast} = B^{-1}b$ the current basic variables' values.
                      Find the largest $t$ such that \( x_B^{\ast} - td \geq 0\)
                      if there is no such $t$, then the problem is unbounded; otherwise, at least
                      one component of  \( x_B^{\ast} - td \) equals zero and the corresponding variable is leaving the basis.
                  \item[\textit{Step 5:}] Set the value of the entering variable at
                      $t$ and replace the values $x_B^{\ast}$ of the basic variables by \( x_B^{\ast} - td \).
                      Replace the leaving column of B by the entering column, and in the basis heading,
                      replace the leaving variable by the entering variable.
              \end{itemize}

        \item \textbf{Return} Optimal solution \( B^{-1}b \)
    \end{enumerate}
\end{algorithm}\label{algo:revised}

\subsubsection*{The product form inverse update method}
We will discuss the PFI, introduced by George Dantzig \parencite{dantzig1954product}.
The revised simplex method needs a way to represent the inverse of the
basis matrix in each step, this is because, when
finding the entering and leaving variables in each iteration,
we need the inverse $B^{-1}$ to solve the \gls{btran}
and \gls{ftran} systems in Step 1 and 3 in \ref{algo:revised}.
Having to reinvert the basis matrix in each step is costly, which is why
an INVERT operation is applied only once at the beginning, on the initial basis
$B_0$. Inverting a matrix using LU decomposition requires $O(n^3)$.
a representaion of the inverse of the basis is kept tr
Given a basis matrix \( B \) and its inverse \( B^{-1} \),
suppose $p$ is the index of the basic variable leaving the basis at this step,
and the vector $a_q$ is the entering column, or the solution of the \gls{ftran}
system, see \ref{algo:revised} Step 3.
\begin{align*}
    \hat{B} & = B + (a_q - B e_p) e_p^T     \\
            & = B (I + (a_q - B e_p) e_p^T) \\
            & = B E
\end{align*}
where \( E = I + (a_q - B e_p) e_p^T \) is an \textit{eta matrix}.


\subsubsection*{The modified product form inverse}

\subsubsection*{Forrest-Tomlin update form}

\subsection{Cardinality Estimation}\label{subsection:cardinality-estimate}
An important use case of linear programming solvers in the field of databases is 
cardinality estimation.
In the context of query optimization, LP solvers can be useful to estimate query plan
cardinalities and provide a reliable and good enough estimate to be used in selecting
the best Join-order, and hence speeding up query execution time.
In the pipeline of query execution, cardinality estimation serves
as a cornerstone for the query optimization process.
Cardinality, defined as the number of tuples in the output,
plays a pivotal role in the selection of an optimal query plan.
Modern \gls{dbms} often rely on
cost-based query optimizers to make this selection.
For example, the SQL Server Query Optimizer
\parencite{microsoft2023cardinality} employs a
cost-based approach, aiming to minimize the estimated
processing cost of executing a query.

Enhanced cardinality estimation
can lead to more accurate cost models, which in turn results
in more efficient query execution plans.
Consequently, accurate and reliable cardinality estimates are
crucial in achieving faster query execution times.
The objective is to develop a \gls{lp}
solver designed specifically for cardinality estimation.
This solver aims to maximize a cost function that represents
the upper bound of the output size, optimizing for both
time and memory complexity.

To set the stage for our implementation,
we focus on the problem of upper-bounding the
cardinality of a join query $Q$.

\subsubsection{Scenario}
To elucidate the core concepts,
suppose we have two relation $R$ and $S$ with attributes
\[
    Q(a, b, c) = R(a, b) \Join S(b, c)
\]
where we denote the sizes of the relations as
$|R|$ and $|S|$ respectively.
It is easy to see that the largest possible output is $|R| \cdot |S|$, which occurs when the join
behaves like a cartesian product, i.e. have a selectivity equals to 1. So, this is the worst-case
upper bound.

\subsubsection{AGM bound}
The AGM bound \parencite{atserias2013size} proves
using entropy that \[ \min_w \left( \sum_{i=1}^{k} w_i \log |R_i| \right) \]
is a tight upper bound for join size, given query graph (how the
relations are connected, if there are any shared attributes)
and relation sizes.
The dual \gls{lp} problem of the given minimzation problem, is
\[ \max \sum_{i} v_i \]
subject to:
\[ A^T \mathbf{v} \leq \log |R| \]
The dual theorem \ref{duality} states that the both problems have the same
optimal values.

This is how our \gls{lp} datasets are generated.

We start with the inequality \ref{eq:initial_inequality}. Applying the natural logarithm to both sides yields \ref{eq:log_inequality}. We then rename the variables, simplifying the inequality to \ref{eq:renamed_inequality}.
Normalizing by dividing both sides by \(r'\), we obtain \ref{eq:normalized_inequality}. This leads us to the objective function for our packing LP problem.
\begin{align}
    |a| \cdot |b|                            & \leq |R| \label{eq:initial_inequality}                                                   \\
    \ln|a| + \ln|b|                          & \leq \ln|R| \label{eq:log_inequality}                                                    \\
    a' + b'                                  & \leq r' \label{eq:renamed_inequality}                                                    \\
    \frac{1}{r'} a' + \frac{1}{r'} b'        & \leq 1 \label{eq:normalized_inequality}                                                  \\
    \text{maximize } a' + b' + c' + d' \quad & \text{s.t.} \quad \frac{1}{r'} a' + \frac{1}{r'} b' \leq 1 \label{eq:objective_function}
\end{align}

And in this simple abstracted way we get a sample packing LP from our dataset.
\subsubsection{Variables}
\subsubsection{Objective}
\subsubsection{Constraints}


\subsection{Other use cases and techniques}
We will focus on one study \parencite{huangfu2015novel}.
\subsection{Other techniques}
The primal simplex method starts from a trial point that is primal feasible and iterates until dual feasibility.
The dual simplex method starts from a trial point that is dual feasible and iterates until primal feasibility.
ALGLIB implements a three-phase dual simplex method with additional degeneracy-breaking perturbation:
\begin{itemize}
    \item Forrest-Tomlin updates for faster LU refactorizations
    \item A bound flipping ratio test (also known as long dual step) for longer steps
    \item Dual steepest edge pricing for better selection of the leaving variable
    \item Shifting (dynamic perturbations applied to cost vector) for better stability
\end{itemize}

\section{State-of-the-art LP solvers}
Here we will discuss alternative approaches that are used today to solve LPs.
\subsection{HIGHS Scipy}
\subsection{Cplex}

